# Qwen3-VL Inference Server Configuration

# Server settings
HOST=0.0.0.0
PORT=8000
DEBUG=False

# Model settings
MODEL_PATH=Qwen/Qwen3-VL-235B-A22B-Instruct

# vLLM settings
GPU_MEMORY_UTILIZATION=0.70
TENSOR_PARALLEL_SIZE=  # Leave empty for auto-detection
MAX_MODEL_LEN=  # Leave empty for default

# Inference settings
DEFAULT_MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.0
DEFAULT_TOP_P=1.0

# Image processing settings
DEFAULT_MIN_PIXELS=65536  # 64 * 32 * 32
DEFAULT_MAX_PIXELS=2097152  # 2048 * 32 * 32

# Video processing settings
DEFAULT_MAX_FRAMES=2048
DEFAULT_SAMPLE_FPS=2.0
DEFAULT_TOTAL_PIXELS=20971520  # 20480 * 32 * 32

# CORS settings (comma-separated)
ALLOW_ORIGINS=*
ALLOW_CREDENTIALS=True
ALLOW_METHODS=*
ALLOW_HEADERS=*
