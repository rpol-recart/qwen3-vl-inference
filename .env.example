# Qwen3-VL Inference Server Configuration

# Server settings
HOST=0.0.0.0
PORT=8000
DEBUG=False

# Model settings
MODEL_PATH=Qwen/Qwen3-VL-4B-Instruct
HF_TOKEN=your_huggingface_token_here  # обязательно для gated модели

# vLLM settings
GPU_MEMORY_UTILIZATION=0.88
TENSOR_PARALLEL_SIZE=  # Leave empty for auto-detection (1 GPU)
MAX_MODEL_LEN=  # Leave empty for default

# Inference settings
DEFAULT_MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.0
DEFAULT_TOP_P=1.0

# Image processing settings
DEFAULT_MIN_PIXELS=65536    # 256x256 equivalent
DEFAULT_MAX_PIXELS=2097152  # ~1440x1440 — safe for FullHD input

# Video processing settings
DEFAULT_MAX_FRAMES=60
DEFAULT_SAMPLE_FPS=12.0
DEFAULT_TOTAL_PIXELS=62914560  # 60M pixels — balance for 60×1080p (with downscaling)

# CORS settings (comma-separated)
ALLOW_ORIGINS=*
ALLOW_CREDENTIALS=True
ALLOW_METHODS=*
ALLOW_HEADERS=*